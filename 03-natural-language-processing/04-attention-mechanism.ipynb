{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMxhOC3gRVH7A0CXsKoSxNt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# üßê Attention is All You Need? Understanding the Attention Mechanism\n","\n","In our previous notebooks, we explored RNNs and Seq2Seq models.\n","\n","However, the world of NLP was revolutionized in 2017 by the paper **\"Attention Is All You Need\"** (Vaswani et al.). This paper introduced a new model, the **Transformer**, which *completely* discarded RNNs and relied *entirely* on a more powerful form of attention called **self-attention**.\n","\n","This notebook is dedicated to understanding this mechanism. We will build, from scratch, the core components of Transformer-style attention:\n","1.  **The \"Query, Key, Value\" Concept:** The high-level theory.\n","2.  **Scaled Dot-Product Attention:** The core mathematical engine.\n","3.  **Multi-Head Attention (MHA):** The powerful, parallelized version that gives the Transformer its strength.\n","4.  **Putting it Together:** We'll build a full `EncoderBlock` to see how MHA is used in practice.\n","\n","Our goal is to *isolate* and *understand* these components before we build a full Transformer model."],"metadata":{"id":"LFLt4RdscUL7"}},{"cell_type":"markdown","source":["## 1. Setup and Imports\n","\n","We'll be using PyTorch to build our components."],"metadata":{"id":"ndoy4k73ciE4"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98yeU-cPVPBI","executionInfo":{"status":"ok","timestamp":1762957351381,"user_tz":-60,"elapsed":30,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"f0c724d7-843e-4444-c8d3-c510851f4602"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.8.0+cu126\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x782324238c10>"]},"metadata":{},"execution_count":12}],"source":["import torch\n","import torch.nn as nn\n","import math\n","import torch.nn.functional as F\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","\n","# Set a seed for reproducible results\n","SEED = 1234\n","torch.manual_seed(SEED)"]},{"cell_type":"markdown","source":["## 2. The \"Query, Key, Value\" Analogy\n","\n","How does \"self-attention\" work? How does a sequence \"attend to itself\"?\n","\n","The \"Query, Key, Value\" (QKV) model is the key. Think of it like a search engine or a database retrieval.\n","\n","Imagine you are at a library.\n","* **Query (Q):** This is your **question**. For a word, the Query is its \"request for information.\" For example, the word \"it\" might have a Query like, \"I'm a pronoun... what noun do I refer to?\"\n","* **Key (K):** This is the **label on a book's spine**. Every word in the sequence has a \"Key\" that describes what it is. The word \"car\" might have a Key like, \"I am a singular, non-human noun.\"\n","* **Value (V):** This is the **actual content of the book**. Every word also has a \"Value\" representing its actual meaning or content.\n","\n","**The Self-Attention Process:**\n","1.  **Compare:** For a single word (let's call it \"Word 1\"), its **Query (Q1)** is compared against *every other word's* **Key (K1, K2, K3...)**. This comparison (a dot product) generates a \"similarity score\" or \"attention score.\"\n","2.  **Get Weights:** These scores are passed through a **softmax** function. This turns the scores into a probability distribution (weights that sum to 1). This is the \"attention\" ‚Äì a set of weights showing how much \"Word 1\" should care about all the other words.\n","3.  **Get Output:** The final output for \"Word 1\" is a **weighted sum** of all the **Values (V1, V2, V3...)** in the sequence, based on the weights calculated in step 2.\n","\n","\n","\n","In essence, the output for each word is a new vector, \"blended\" from all other words in the sequence, based on *how relevant* they are (Q-K similarity)."],"metadata":{"id":"7cbzZmU_cnoW"}},{"cell_type":"markdown","source":["## 3. Part 1: Scaled Dot-Product Attention\n","\n","This is the core engine that implements the QKV logic. It's defined by the following equation:\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n","$$\n","\n","Let's break this down:\n","\n","1.  **$QK^T$**: This is the \"Compare\" step. We perform a matrix multiplication between all Queries and all Keys (transposed). This efficiently calculates the \"similarity score\" between every query and every key.\n","    * If `Q` is `[seq_len_q, d_k]` and `K` is `[seq_len_k, d_k]`, then `K^T` is `[d_k, seq_len_k]`.\n","    * The result `QK^T` is a matrix of shape `[seq_len_q, seq_len_k]` containing all the scores.\n","\n","2.  **$\\frac{...}{\\sqrt{d_k}}$**: This is the \"Scaled\" part.\n","    * `d_k` is the dimension of the Key vectors.\n","    * **Why scale?** For large values of `d_k`, the dot products $QK^T$ can become very large. This pushes the softmax function into regions with tiny gradients (it gets \"saturated\").\n","    * By dividing by the square root of `d_k`, we \"tame\" these values, keeping the gradients healthy and making training more stable.\n","\n","3.  **$\\text{softmax}(...)$**: This is the \"Get Weights\" step. We apply softmax along the \"keys\" dimension. This converts the raw scores into a probability distribution (weights) for each query.\n","\n","4.  **$...V$**: This is the \"Get Output\" step. We multiply our weights (a `[seq_len_q, seq_len_k]` matrix) by the Value matrix (`[seq_len_k, d_v]`). This gives us a final, weighted-sum output of shape `[seq_len_q, d_v]`.\n","\n","Let's implement this."],"metadata":{"id":"4O8RNCspdAUM"}},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value, mask=None):\n","    \"\"\"\n","    Computes Scaled Dot-Product Attention.\n","\n","    Args:\n","        query (Tensor): Shape [batch_size, n_heads, seq_len_q, d_k]\n","        key (Tensor): Shape [batch_size, n_heads, seq_len_k, d_k]\n","        value (Tensor): Shape [batch_size, n_heads, seq_len_k, d_v]\n","        mask (Tensor, optional): Shape [batch_size, 1, 1, seq_len_k] or [batch_size, 1, seq_len_q, seq_len_k]\n","\n","    Returns:\n","        output (Tensor): Shape [batch_size, n_heads, seq_len_q, d_v]\n","        attn_weights (Tensor): Shape [batch_size, n_heads, seq_len_q, seq_len_k]\n","    \"\"\"\n","\n","    # query, key, and value dimensions are d_k (or d_q) and d_v\n","    # For self-attention, d_k = d_v\n","    d_k = query.size(-1)\n","\n","    # 1. MatMul(Q, K^T)\n","    # (batch_size, n_heads, seq_len_q, d_k) @ (batch_size, n_heads, d_k, seq_len_k)\n","    # -> (batch_size, n_heads, seq_len_q, seq_len_k)\n","    scores = torch.matmul(query, key.transpose(-2, -1))\n","\n","    # 2. Scale\n","    scores = scores / math.sqrt(d_k)\n","\n","    # 3. Mask (Optional)\n","    # Masks are used to hide certain positions from the attention.\n","    # e.g., padding tokens, or future tokens in a decoder.\n","    if mask is not None:\n","        # We broadcast the mask by adding a very small number (e.g., -1e9)\n","        # to the scores. Softmax(x + -inf) = 0.\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","\n","    # 4. Softmax\n","    attn_weights = F.softmax(scores, dim=-1)\n","\n","    # 5. MatMul(weights, V)\n","    # (batch_size, n_heads, seq_len_q, seq_len_k) @ (batch_size, n_heads, seq_len_k, d_v)\n","    # -> (batch_size, n_heads, seq_len_q, d_v)\n","    output = torch.matmul(attn_weights, value)\n","\n","    return output, attn_weights\n","\n","# --- Let's test it with some dummy data ---\n","\n","# We'll use 4 dimensions, 1 head, batch size 1, for simplicity\n","# (batch_size, n_heads, seq_len, d_k/d_v)\n","B, H, L, D = 1, 1, 3, 4 # Batch=1, Head=1, SeqLen=3, Dim=4\n","\n","# Create 3 \"words\"\n","q = torch.rand(B, H, L, D)\n","k = torch.rand(B, H, L, D)\n","v = torch.rand(B, H, L, D)\n","\n","print(\"Query shape:\", q.shape)\n","\n","output, weights = scaled_dot_product_attention(q, k, v)\n","\n","print(\"\\nOutput shape:\", output.shape)\n","print(\"Attention Weights shape:\", weights.shape)\n","print(\"\\nAttention Weights (summed over last dim):\")\n","print(weights.sum(dim=-1)) # Should sum to 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNVyogyGVTGZ","executionInfo":{"status":"ok","timestamp":1762957633482,"user_tz":-60,"elapsed":73,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"3825de9c-94a8-40cb-8d28-e7f42784a34d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Query shape: torch.Size([1, 1, 3, 4])\n","\n","Output shape: torch.Size([1, 1, 3, 4])\n","Attention Weights shape: torch.Size([1, 1, 3, 3])\n","\n","Attention Weights (summed over last dim):\n","tensor([[[1., 1., 1.]]])\n"]}]},{"cell_type":"markdown","source":["## 4. Part 2: Multi-Head Attention\n","\n","Why just do this once? The model might need to ask multiple questions simultaneously.\n","\n","* When looking at \"it,\" one part of the model (one \"head\") might ask, \"What noun does this refer to?\"\n","* Another head might ask, \"Is this part of a possessive phrase?\"\n","* Another might ask, \"Is this a subject or an object?\"\n","\n","**Multi-Head Attention (MHA)** runs the Scaled Dot-Product Attention mechanism multiple times (*h* times) in parallel.\n","\n","**Process:**\n","1.  **Project:** Take the *single* input `Query`, `Key`, and `Value` and pass them through *h* independent Linear layers (Wq, Wk, Wv) to create *h* different sets of Q, K, and V vectors.\n","2.  **Split:** We don't actually do *h* separate linear layers. Instead, we use one *large* Linear layer (e.g., from `d_model` to `d_model`) and then \"split\" the resulting `d_model` vector into *h* \"heads.\"\n","    * If `d_model = 512` and `h = 8`, we split the 512-dim vector into 8 chunks of 64 dimensions. Each chunk is one \"head.\"\n","3.  **Attend:** Run Scaled Dot-Product Attention on each head *in parallel*. Each head gets its own output and attention weights.\n","4.  **Concatenate:** Re-combine the outputs of all *h* heads (e.g., concatenate the 8 chunks of 64-dim vectors back into one 512-dim vector).\n","5.  **Final Linear Layer:** Pass this concatenated vector through one final linear layer (Wo) to mix the information from all heads.\n","\n","\n","\n","This allows each head to \"attend\" to different parts of the input, learning different types of relationships."],"metadata":{"id":"c2glcSkfec2f"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, d_model, h):\n","        \"\"\"\n","        Args:\n","            d_model (int): The dimensionality of the input/output.\n","            h (int): The number of attention heads.\n","        \"\"\"\n","        super().__init__()\n","\n","        assert d_model % h == 0, \"d_model must be divisible by h\"\n","\n","        self.d_model = d_model\n","        self.h = h\n","        self.d_k = d_model // h # Dimension of each head's Key\n","        self.d_v = d_model // h # Dimension of each head's Value\n","\n","        # 1. Define the big Linear layers for Q, K, V\n","        # These will project d_model -> d_model\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","\n","        # 5. Define the final Linear layer (Wo)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        Splits the last dimension (d_model) into h heads.\n","        Shape: (batch_size, seq_len, d_model) -> (batch_size, h, seq_len, d_k)\n","        \"\"\"\n","        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, h, d_k)\n","        x = x.view(batch_size, -1, self.h, self.d_k)\n","        # (batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n","        return x.transpose(1, 2)\n","\n","    def forward(self, query_in, key_in, value_in, mask=None):\n","        # In self-attention, query_in, key_in, and value_in are all the same tensor.\n","        # But for encoder-decoder attention, they can be different.\n","\n","        batch_size = query_in.size(0)\n","\n","        # 1. Project through Wq, Wk, Wv\n","        # (batch_size, seq_len_q, d_model)\n","        Q = self.W_q(query_in)\n","        # (batch_size, seq_len_k, d_model)\n","        K = self.W_k(key_in)\n","        # (batch_size, seq_len_k, d_model)\n","        V = self.W_v(value_in)\n","\n","        # 2. Split heads\n","        # (batch_size, h, seq_len_q, d_k)\n","        Q = self.split_heads(Q, batch_size)\n","        # (batch_size, h, seq_len_k, d_k)\n","        K = self.split_heads(K, batch_size)\n","        # (batch_size, h, seq_len_k, d_v)\n","        V = self.split_heads(V, batch_size)\n","\n","        # 3. Scaled Dot-Product Attention\n","        # `context` shape: (batch_size, h, seq_len_q, d_v)\n","        # `attn_weights` shape: (batch_size, h, seq_len_q, seq_len_k)\n","        context, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n","\n","        # 4. Concatenate heads\n","        # (batch_size, h, seq_len_q, d_v) -> (batch_size, seq_len_q, h, d_v)\n","        context = context.transpose(1, 2).contiguous()\n","        # (batch_size, seq_len_q, h, d_v) -> (batch_size, seq_len_q, d_model)\n","        context = context.view(batch_size, -1, self.d_model)\n","\n","        # 5. Pass through final linear layer (Wo)\n","        # (batch_size, seq_len_q, d_model)\n","        output = self.W_o(context)\n","\n","        return output, attn_weights\n","\n","# --- Let's test it! ---\n","D_MODEL = 512\n","HEADS = 8\n","BATCH_SIZE = 32\n","SEQ_LEN = 10 # 10 \"words\" in our sentence\n","\n","# Create a MHA module\n","mha = MultiHeadAttention(d_model=D_MODEL, h=HEADS)\n","\n","# Create a dummy input tensor (batch, seq_len, d_model)\n","# This is \"self-attention\" so Q, K, and V all come from the same source `x`\n","x = torch.rand(BATCH_SIZE, SEQ_LEN, D_MODEL)\n","\n","# Pass it through the MHA\n","output, weights = mha(query_in=x, key_in=x, value_in=x)\n","\n","print(f\"Input shape (x): {x.shape}\")\n","print(f\"Output shape: {output.shape}\")\n","print(f\"Attention weights shape: {weights.shape}\")\n","\n","# Check the weights shape: (batch_size, h, seq_len_q, seq_len_k)\n","assert weights.shape == (BATCH_SIZE, HEADS, SEQ_LEN, SEQ_LEN)\n","# Check the output shape: (batch_size, seq_len_q, d_model)\n","assert output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwhuPk3iVVGO","executionInfo":{"status":"ok","timestamp":1762958890121,"user_tz":-60,"elapsed":34,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"6400c055-1dc3-46f4-df3b-9a7b8ca122fa"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape (x): torch.Size([32, 10, 512])\n","Output shape: torch.Size([32, 10, 512])\n","Attention weights shape: torch.Size([32, 8, 10, 10])\n"]}]},{"cell_type":"markdown","source":["## 5. Part 3: How is this used? (A Transformer Encoder Block)\n","\n","So we have this powerful `MultiHeadAttention` module. How is it used?\n","\n","In a Transformer, it's used inside an **Encoder Block**. This block has two main parts:\n","1.  **Multi-Head Attention:** The part we just built. This allows the sequence to look at itself.\n","2.  **Position-wise Feed-Forward Network:** A simple 2-layer MLP that is applied independently to *each token* (position).\n","\n","It also uses two crucial components for deep learning:\n","* **Residual Connections:** `x + Sublayer(x)`. We add the *input* to the *output* of the sub-layer. This helps prevent the vanishing gradient problem.\n","* **Layer Normalization:** This normalizes the features across the `d_model` dimension for each token independently. It provides stability during training.\n","\n","The structure is:\n","1.  **Input:** `x`\n","2.  `x_attn = MultiHeadAttention(x)`\n","3.  `x_norm1 = LayerNorm(x + x_attn)` (Add & Norm)\n","4.  `x_ffn = FeedForwardNetwork(x_norm1)`\n","5.  `output = LayerNorm(x_norm1 + x_ffn)` (Add & Norm)"],"metadata":{"id":"fWvq5mENi9vY"}},{"cell_type":"code","source":["class PositionWiseFeedForward(nn.Module):\n","    \"\"\" A simple 2-layer MLP for the Encoder/Decoder blocks \"\"\"\n","    def __init__(self, d_model, d_ff):\n","        super().__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model)\n","        return self.linear2(self.relu(self.linear1(x)))\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\" A single Transformer Encoder Block \"\"\"\n","    def __init__(self, d_model, h, d_ff, dropout=0.1):\n","        super().__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, h)\n","        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n","\n","        self.layernorm1 = nn.LayerNorm(d_model)\n","        self.layernorm2 = nn.LayerNorm(d_model)\n","\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","\n","        # 1. Multi-Head Attention sub-layer\n","        attn_output, _ = self.mha(x, x, x, mask)\n","\n","        # 2. Add & Norm\n","        # x + dropout(attn_output)\n","        x = self.layernorm1(x + self.dropout1(attn_output))\n","\n","        # 3. Feed-Forward sub-layer\n","        ffn_output = self.ffn(x)\n","\n","        # 4. Add & Norm\n","        # x + dropout(ffn_output)\n","        output = self.layernorm2(x + self.dropout2(ffn_output))\n","\n","        return output\n","\n","# --- Let's test the full block ---\n","D_MODEL = 512\n","HEADS = 8\n","D_FF = 2048 # Standard in the paper\n","BATCH_SIZE = 32\n","SEQ_LEN = 10\n","\n","# Create a dummy input tensor\n","x = torch.rand(BATCH_SIZE, SEQ_LEN, D_MODEL)\n","\n","# Create an encoder block\n","encoder_block = EncoderBlock(D_MODEL, HEADS, D_FF)\n","\n","# Pass the input through\n","output = encoder_block(x)\n","\n","print(f\"Input shape (x): {x.shape}\")\n","print(f\"Final output shape: {output.shape}\")\n","\n","# The output shape should be identical to the input shape!\n","assert output.shape == x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mcx9eanRVXan","executionInfo":{"status":"ok","timestamp":1762959177976,"user_tz":-60,"elapsed":85,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"f7f8b463-fb02-48b1-d2c5-a764027646af"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape (x): torch.Size([32, 10, 512])\n","Final output shape: torch.Size([32, 10, 512])\n"]}]},{"cell_type":"markdown","source":["## 6. Conclusion and Next Steps\n","\n","We've done it! We have successfully built the core components of the Transformer model from the ground up.\n","\n","* We started with the **QKV concept**.\n","* We implemented the math in **Scaled Dot-Product Attention**.\n","* We parallelized it and made it powerful with **Multi-Head Attention**.\n","* We saw how it fits into a full **Encoder Block** with Layer Normalization and Feed-Forward networks.\n","\n","You now understand the \"attention\" in \"Attention Is All You Need.\"\n","\n","**Next Steps:**\n","* **Positional Encodings:** One thing we've ignored is *word order*. Our MHA is \"permutation-invariant\" (shuffling the words would give the same result, just shuffled). The next step is to learn about Positional Encodings, which inject word-order information.\n","* **Building a Full Transformer:** We can now stack several `EncoderBlock`s to build a full Encoder, and then build a Decoder (which uses *masked* MHA) to create a full Encoder-Decoder Transformer model for machine translation.\n","* **BERT / GPT:** You now have the foundational block for understanding modern pre-trained models. BERT is essentially a stack of these Encoder Blocks."],"metadata":{"id":"YZ7ArK_pjsa9"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPycWf8EIzQHg6eWtw8sNS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# ü§ñ Beyond the Vanilla Transformer: Meet the Family (BERT, GPT, T5)\n","\n","In our last notebook, we built the original \"vanilla\" Transformer, the Encoder-Decoder model introduced in \"Attention Is All You Need.\" This architecture was a revolution.\n","\n","However, researchers soon discovered that the **Encoder** and **Decoder** components were incredibly powerful on their own. By isolating and modifying them, they created a \"zoo\" of new models, each specialized for different tasks.\n","\n","This notebook is a practical tour of the three main families of Transformer-based models. We won't build them from scratch; instead, we'll use the powerful **Hugging Face `transformers` library** to load pre-trained versions and understand:\n","\n","1.  **Encoder-Only (like BERT):** The \"Understander.\"\n","2.  **Decoder-Only (like GPT):** The \"Generator.\"\n","3.  **Encoder-Decoder (like T5):** The \"Translator\" or \"Swiss Army Knife.\"\n","\n","**Our Goal:** To understand the conceptual differences between these architectures and see practical examples of what each is best suited for."],"metadata":{"id":"Eaph3qG6PX6Z"}},{"cell_type":"markdown","source":["## 1. Setup: Installing the Transformers Library\n","\n","First, we need to install the `transformers` library from Hugging Face. This library gives us easy access to thousands of pre-trained models. We'll also install `datasets` for some examples."],"metadata":{"id":"aoPggT6rQDgC"}},{"cell_type":"code","source":["!pip install -q transformers datasets\n","!pip install -q sentencepiece # Required for T5\n","\n","import torch\n","from transformers import pipeline\n","\n","print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-LU0Zt3QFm5","executionInfo":{"status":"ok","timestamp":1763289852535,"user_tz":-60,"elapsed":45856,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"b91e4404-6ab6-4cab-8dfc-e00281549cae"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["## 2. A High-Level Overview\n","\n","Before we dive in, here's the key difference in one sentence:\n","\n","* **Encoder-Only (BERT):** Sees the *entire* input sentence at once (bidirectional). It's pre-trained to \"fill in the blanks,\" so it's a master of understanding context.\n","* **Decoder-Only (GPT):** Sees only the text that came *before* it (left-to-right). It's pre-trained to \"predict the next word,\" so it's a master of generation.\n","* **Encoder-Decoder (T5):** Has both. The encoder \"understands\" the input, and the decoder \"generates\" the output based on that understanding."],"metadata":{"id":"1Qpumuy0Qma2"}},{"cell_type":"markdown","source":["## 3. üß† Encoder-Only: The \"Understander\" (BERT)\n","\n","The **BERT** (Bidirectional Encoder Representations from Transformers) family models are essentially just the **Encoder** stack from the original Transformer.\n","\n","### How it Works:\n","* **Context:** **Bidirectional.** When processing a word, a BERT-style model can \"see\" all the words that come *before* it and *after* it in the sequence. This is its superpower.\n","* **Pre-training:** BERT is pre-trained using **Masked Language Modeling (MLM)**. This means it's given sentences with random words \"masked out\" (e.g., `The [MASK] sat on the mat.`) and its job is to use the full context to guess the masked word. This makes it an expert at building rich, contextual \"understanding\" of a sentence.\n","\n","\n","\n","### What it's Best For:\n","Tasks that require a deep understanding of the *entire* sentence. These are often called **Natural Language Understanding (NLU)** tasks.\n","\n","* **Sentiment Analysis:** Is this review positive or negative?\n","* **Text Classification:** What topic is this news article about?\n","* **Named Entity Recognition (NER):** Find all the \"people,\" \"places,\" and \"organizations\" in this text.\n","* **Extractive Question Answering:** Given a text, find the *exact span* of text that answers a question.\n","\n","### Practical Examples\n","\n","We can use the Hugging Face `pipeline` to easily use these models.\n","\n","#### Example 1: Masked Language Model (BERT's \"native\" task)\n","Let's see BERT do what it was trained to do: fill in the blanks."],"metadata":{"id":"9mgJDFgRQ2KN"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXt_JudZPV1Z","executionInfo":{"status":"ok","timestamp":1763289969800,"user_tz":-60,"elapsed":857,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"c9a76999-799d-4fe0-a9ec-c1211686485d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["\n","Original: The capital of France is [MASK].\n","\n","Token: paris      | Score: 0.4168\n","Token: lille      | Score: 0.0714\n","Token: lyon       | Score: 0.0634\n","Token: marseille  | Score: 0.0444\n","Token: tours      | Score: 0.0303\n"]}],"source":["# Load a fill-mask pipeline with a BERT model\n","# 'bert-base-uncased' is a good, standard model\n","fill_mask = pipeline('fill-mask', model='bert-base-uncased')\n","\n","sentence = \"The capital of France is [MASK].\"\n","\n","results = fill_mask(sentence)\n","\n","print(f\"\\nOriginal: {sentence}\\n\")\n","for result in results:\n","    print(f\"Token: {result['token_str']:<10} | Score: {result['score']:.4f}\")"]},{"cell_type":"markdown","source":["> **Observation:** Notice how it knows \"paris\" is the most likely word, but it also suggests other plausible (but incorrect) city names. It's using the context of \"capital\" and \"France.\"\n","\n","#### Example 2: Sentiment Analysis (A common fine-tuned task)\n","Here, a BERT-style model has been \"fine-tuned\" on a labeled dataset for sentiment analysis. The core model \"understands\" the sentence, and a small classification \"head\" is added on top to output \"Positive\" or \"Negative.\""],"metadata":{"id":"YDzxVqm1ReRX"}},{"cell_type":"code","source":["# We'll use 'distilbert', a smaller, faster version of BERT.\n","classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n","\n","text1 = \"I love this notebook! It's so clear and helpful.\"\n","text2 = \"This is the worst movie I have ever seen. It was boring and poorly acted.\"\n","\n","print(classifier(text1))\n","print(classifier(text2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWzJ8eDdRmgI","executionInfo":{"status":"ok","timestamp":1763290027123,"user_tz":-60,"elapsed":454,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"02f83c96-136c-4e78-f067-c95c17090042"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.999875545501709}]\n","[{'label': 'NEGATIVE', 'score': 0.999819815158844}]\n"]}]},{"cell_type":"markdown","source":["## 4. ‚úçÔ∏è Decoder-Only: The \"Generator\" (GPT)\n","\n","The **GPT** (Generative Pre-trained Transformer) family models are essentially just the **Decoder** stack from the original Transformer.\n","\n","### How it Works:\n","* **Context:** **Causal (or \"Masked\")**. This is the key. A Decoder-Only model can only see words to its *left* (the past). It *cannot* see future words. This is enforced by the \"look-ahead mask\" we built in the Transformer notebook.\n","* **Pre-training:** GPT is pre-trained on a simple **Causal Language Modeling** task: predict the *very next word* in a sequence. It's fed massive amounts of text from the internet and just learns to predict what comes next, over and over.\n","\n","\n","\n","### What it's Best For:\n","Tasks that involve creating new, coherent text from a prompt. These are called **Natural Language Generation (NLG)** tasks.\n","\n","* **Text Generation:** Continue a story, write a poem, complete a prompt.\n","* **Chatbots:** Generating a response in a conversation.\n","* **Summarization (Abstractive):** Writing a *new* summary (not just copying sentences).\n","* **Generative Question Answering:** Answering a question by generating a new sentence.\n","\n","### Practical Example\n","\n","Let's use the `pipeline` to generate text with **GPT-2**, a famous model from this family.\n","\n","#### Example 1: Text Generation"],"metadata":{"id":"vG0nxXsiRxmh"}},{"cell_type":"code","source":["# Load a text-generation pipeline with GPT-2\n","generator = pipeline('text-generation', model='gpt2')\n","\n","prompt = \"In a world where dragons and magic are real,\"\n","\n","# Let's generate 2 different (and longer) completions\n","results = generator(prompt, max_length=50, num_return_sequences=2)\n","\n","for i, result in enumerate(results):\n","    print(f\"--- Completion {i+1} ---\")\n","    print(result['generated_text'])\n","    print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6m__gB_sSeLG","executionInfo":{"status":"ok","timestamp":1763290267444,"user_tz":-60,"elapsed":3722,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"8f262f0b-7abb-4a0b-b5d3-d9df502199c1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["--- Completion 1 ---\n","In a world where dragons and magic are real, it may seem like the best option, as we can see from the video below.\n","\n","The next thing is to see whether the dragons or magic of the world actually work. The video shows them in their natural state. The video shows them in their magic state.\n","\n","It's not clear what makes these dragons, the magic of the world, work. But if the ability to fly is a thing you can see in the video, then the ability to fly is probably better than the ability to fly. The dragons may be able to fly in a different way, but they're still dragons.\n","\n","In the video, the dragon is shown flying in a way that is the same as its natural state, a way that doesn't involve a lot of movement. But it does have a move. It also has a move that is in real time, such as a circle that changes direction.\n","\n","The video does show dragons moving in a way that doesn't involve a lot of movement. But it does have a move that is in real time, such as a circle that changes direction.\n","\n","The dragon has an eye. It has a long nose. The dragon has a large tail and a small tail.\n","\n","The dragon has a large tail and a\n","\n","\n","--- Completion 2 ---\n","In a world where dragons and magic are real, the only way to make these dragons even more powerful is to use magic, and to use magic, and to use magic, and to use magic, and to use magic, and to use magic.\n","\n","So why does magic work? Well, it's very simple: magic is the ability to create a random number. So when you create a number, you can create a random number. So when you create a number, you can create a random number. And magic is the ability to create a random number.\n","\n","Magic is the ability to create a random number.\n","\n","And magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","Magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","Magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number.\n","\n","So magic is the ability to create a random number\n","\n","\n"]}]},{"cell_type":"markdown","source":["> **Observation**: Notice how the model \"auto-regressively\" generates text, token by token, with each new word being conditioned on the words that came before it. It's just \"predicting the next word\" on a loop."],"metadata":{"id":"0Dq3HaKXS-_d"}},{"cell_type":"markdown","source":["## 5. üîÅ Encoder-Decoder: The \"Swiss Army Knife\" (T5)\n","\n","This family brings us back to the original architecture, but with a twist. Models like **T5** (Text-to-Text Transfer Transformer) and **BART** use *both* the Encoder and Decoder.\n","\n","### How it Works:\n","* **Context:** The Encoder has **bidirectional** context (like BERT) to \"understand\" the input. The Decoder has **causal** context (like GPT) to \"generate\" the output.\n","* **Pre-training:** T5's clever idea is to frame *every* NLP task as a **\"text-to-text\"** problem. It's pre-trained by taking text, corrupting it (e.g., `The cat <X> on the <Y> mat.`), and training the model to re-generate the original text (e.g., `The cat sat on the <Z> mat.` -> `T5 output: <X> sat <Y> <Z>`).\n","* **Task Prefixes:** To tell the model *what* to do, you add a **prefix** to the input text.\n","    * `\"summarize: [article text]...\"`\n","    * `\"translate English to German: [English text]...\"`\n","    * `\"cola sentence: [sentence]...\"` (for a classification task)\n","\n","\n","\n","### What it's Best For:\n","Tasks that **transform** an input sequence into a *new* output sequence. This makes them incredibly versatile.\n","\n","* **Machine Translation:** (The original Transformer task).\n","* **Abstractive Summarization:** (Often the best models for this).\n","* **Generative Question Answering:** Taking a question and context and generating a free-form answer.\n","\n","### Practical Examples\n","\n","Let's use **T5-small** (a smaller version) to perform two different tasks with the *same* model, just by changing the pipeline.\n","\n","#### Example 1: Summarization"],"metadata":{"id":"Sg7OtOn9TGbM"}},{"cell_type":"code","source":["# Load a summarization pipeline with T5\n","summarizer = pipeline('summarization', model='t5-small')\n","\n","long_text = \"\"\"\n","The Transformer is a deep learning model architecture introduced in 2017.\n","It is notable for its use of self-attention mechanisms, which allow it to weigh the\n","importance of different words in a sequence. Unlike Recurrent Neural Networks (RNNs),\n","the Transformer does not process data sequentially, enabling significant parallelization\n","and faster training. This architecture is split into an Encoder and a Decoder.\n","The Encoder's job is to build a rich, contextual representation of the input, while\n","the Decoder's job is to generate an output sequence based on that representation.\n","This design proved to be highly effective for machine translation and has since become\n","the foundation for most modern large language models, including BERT and GPT.\n","\"\"\"\n","\n","summary = summarizer(long_text, max_length=50, min_length=10)\n","print(f\"\\n{summary[0]['summary_text']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFEvdx8mUMG4","executionInfo":{"status":"ok","timestamp":1763290827285,"user_tz":-60,"elapsed":1380,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"339b931d-f8d1-494b-bd74-0b6abc52e268"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"output_type":"stream","name":"stdout","text":["\n","the Transformer is a deep learning model architecture introduced in 2017 . it uses self-attention mechanisms to weigh the importance of different words in a sequence . the Transformer does not process data sequentially, enabling significant parallelization and faster training .\n"]}]},{"cell_type":"markdown","source":["#### Example 2: Translation\n","Now, we use the exact same model architecture, but one that's been fine-tuned for translation. The pipeline handles the correct prefix (`\"translate English to German: \"`) for us."],"metadata":{"id":"zsG8iKIgU8ML"}},{"cell_type":"code","source":["# Load a translation pipeline, also using a T5 model\n","translator = pipeline('translation_en_to_de', model='t5-small')\n","\n","text = \"This notebook is a great introduction to the different Transformer architectures.\"\n","\n","translation = translator(text)\n","print(f\"\\n{translation[0]['translation_text']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKsGAW8rVPZ-","executionInfo":{"status":"ok","timestamp":1763291002635,"user_tz":-60,"elapsed":1033,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"96c04460-df0c-4ecd-d735-d11c85180ebf"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["\n","Dieses Notebook ist eine gro√üartige Einf√ºhrung in die verschiedenen Transformer-Architekturen.\n"]}]},{"cell_type":"markdown","source":["## 6. Summary: A Quick Comparison\n","\n","Here's a simple table to help you remember the differences.\n","\n","| Feature | Encoder-Only (e.g., BERT) | Decoder-Only (e.g., GPT) | Encoder-Decoder (e.g., T5) |\n","| :--- | :--- | :--- | :--- |\n","| **Full Name** | Bidirectional Encoder | Causal Decoder | Encoder-Decoder |\n","| **Context** | **Bidirectional** (sees all) | **Causal** (sees past) | Bidirectional (Encoder), Causal (Decoder) |\n","| **Analogy** | The \"Understander\" | The \"Generator\" | The \"Translator\" / \"Transformer\" |\n","| **Pre-training** | Masked Language Model (MLM) | Causal Language Model (Next Token) | \"Text-to-Text\" (Span Corruption) |\n","| **Best For...** | **NLU** (Understanding) | **NLG** (Generating) | **Seq2Seq** (Transforming) |\n","| **Examples** | Classification, NER | Chatbots, Story Writing | Translation, Summarization |\n","| **Models** | BERT, RoBERTa, ALBERT | GPT, LLaMA, Mistral | T5, BART, \"Vanilla\" Transformer |"],"metadata":{"id":"5aB1VSb1VPL3"}},{"cell_type":"markdown","source":["## 7. Conclusion\n","\n","You've now met the three main families of Transformer models and seen them in action.\n","\n","The key takeaway is that **the architecture dictates the task.**\n","* Need to **understand** a sentence for classification? Use an **Encoder (BERT)**.\n","* Need to **generate** creative text from a prompt? Use a **Decoder (GPT)**.\n","* Need to **transform** an input sequence into a new one? Use an **Encoder-Decoder (T5)**.\n","\n","This powerful ecosystem, all stemming from the original 2017 paper, is what powers almost all of modern NLP."],"metadata":{"id":"-dAKuKQSVuyk"}}]}
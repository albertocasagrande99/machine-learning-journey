{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNZl4BLAqMH98U+Lsup2MUo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# üß† From A to B: Understanding Sequence-to-Sequence Models\n","\n","Welcome to this notebook on **Sequence-to-Sequence (Seq2Seq)** models! This is a cornerstone architecture in modern NLP, forming the basis for everything from machine translation to text summarization.\n","\n","In our previous notebooks, we learned how to preprocess text and create word embeddings. Now, we'll use those concepts to build a model that can \"read\" a sequence (like an English sentence) and \"write\" a new sequence (like a French sentence).\n","\n","**Our Goal:** To build a simple English-to-French translator from scratch using a \"vanilla\" Encoder-Decoder model.\n","\n","**Notebook Outline:**\n","1.  **Introduction:** What is a Seq2Seq model? (The big picture)\n","2.  **Setup:** Importing libraries and preparing our environment.\n","3.  **Data:** Loading, tokenizing, and preparing our parallel text data.\n","4.  **The Architecture:** Building the three key components:\n","    * **The Encoder:** Reads the input sentence.\n","    * **The Decoder:** Generates the output sentence.\n","    * **The Seq2Seq Model:** A wrapper that combines them.\n","5.  **Training:** Teaching the model to translate.\n","6.  **Inference:** Using our trained model to translate new sentences.\n","7.  **Next Steps:** Where to go from here (Attention!)."],"metadata":{"id":"p0XOjD7UXiPd"}},{"cell_type":"markdown","source":["## 1. Introduction: The Big Picture\n","\n","A Sequence-to-Sequence (Seq2Seq) model is designed for tasks where the input and output are both sequences (lists of items), but their lengths may differ.\n","\n","* **Input:** A sequence, e.g., `[ \"hello\", \"how\", \"are\", \"you\", \"?\" ]`\n","* **Output:** A sequence, e.g., `[ \"bonjour\", \"comment\", \"allez\", \"vous\", \"?\" ]`\n","\n","\n","\n","Think of it like a human translator. The translator first **reads** the entire English sentence (this is the **Encoder**). They build a mental \"summary\" or \"understanding\" of its meaning. This summary is what we call the **context vector** (or \"thought vector\").\n","\n","Then, the translator **writes** the French sentence, word by word (this is the **Decoder**). At each step, they consult their mental summary (the context vector) and the word they just wrote to decide which word to write next.\n","\n","Our model will mimic this process:\n","1.  **Encoder (an RNN):** Will \"read\" the input English sentence and compress all its information into a single vector (the final hidden state).\n","2.  **Decoder (another RNN):** Will take that context vector and generate the French sentence token by token.\n","\n","### The Encoder-Decoder Paradigm\n","\n","The Seq2Seq model consists of two main components:\n","\n","```\n","Input Sequence ‚Üí [ENCODER] ‚Üí Context Vector ‚Üí [DECODER] ‚Üí Output Sequence\n","```\n","\n","#### **Encoder**\n","- Takes the input sequence (e.g., English sentence)\n","- Processes it word by word using an RNN (LSTM/GRU)\n","- Compresses all information into a fixed-size **context vector** (also called **thought vector**)\n","- This vector captures the \"meaning\" of the entire input sequence\n","\n","#### **Context Vector**\n","- A fixed-size vector (e.g., 256 or 512 dimensions)\n","- Acts as a bottleneck - all input information must pass through it\n","- Represents the semantic meaning of the input\n","- This is actually the **final hidden state** of the encoder\n","\n","#### **Decoder**\n","- Takes the context vector as its initial hidden state\n","- Generates the output sequence word by word\n","- At each step, it predicts the next word based on:\n","  - The context vector\n","  - Its current hidden state\n","  - The previously generated word"],"metadata":{"id":"PgBdO2OpX1PE"}},{"cell_type":"markdown","source":["## 2. Setup: Imports and Prerequisites\n","\n","Let's import all the libraries we'll need. We'll be using **PyTorch** to build our models and **spaCy** for high-quality text tokenization."],"metadata":{"id":"WqH1OoOCYAs4"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K63eUQNmXeoq","executionInfo":{"status":"ok","timestamp":1762881903728,"user_tz":-60,"elapsed":26766,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"fcbc4b03-2b53-48a3-fa70-aef0a167ab18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Collecting fr-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fr-core-news-sm\n","Successfully installed fr-core-news-sm-3.8.0\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n","\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torch.utils.data import DataLoader\n","\n","# Using spacy for tokenization\n","!pip install -q spacy\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download fr_core_news_sm\n","\n","import spacy\n","\n","import random\n","import math\n","import time\n","from collections import Counter"]},{"cell_type":"markdown","source":["### 2.1. Configure Device (GPU)\n","\n","We'll set up our code to use the GPU, which will make training *significantly* faster."],"metadata":{"id":"9Layljt8YIyA"}},{"cell_type":"code","source":["# Check for GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Set a seed for reproducible results\n","SEED = 1234\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXur4mX_YMhT","executionInfo":{"status":"ok","timestamp":1762881917348,"user_tz":-60,"elapsed":39,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"b8fb5878-431d-443e-feb0-e9733eca0b7f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["## 3. Data Preparation\n","\n","This is one of the most important parts. We need a \"parallel corpus\"‚Äîa dataset with pairs of sentences (e.g., English and its French translation). We'll use a small, clean dataset from [Tatoeba](https://tatoeba.org/en/) for this educational notebook.\n","\n","### 3.1. Download and Load Data"],"metadata":{"id":"6kn03QGiYVge"}},{"cell_type":"code","source":["# Download a small, clean parallel corpus (eng-fra)\n","!wget -q https://www.manythings.org/anki/fra-eng.zip\n","!unzip -q fra-eng.zip\n","\n","# We'll just use the fra.txt file, which contains pairs separated by a tab\n","DATA_PATH = 'fra.txt'"],"metadata":{"id":"KFusQKZpYY1A","executionInfo":{"status":"ok","timestamp":1762881985503,"user_tz":-60,"elapsed":1113,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 3.2. Tokenization\n","\n","We need to break sentences into \"tokens\" (words or punctuation). We'll use spaCy's powerful tokenizers for English and French."],"metadata":{"id":"MQi6Xy2WYLiw"}},{"cell_type":"code","source":["# Load spacy models\n","spacy_en = spacy.load('en_core_web_sm')\n","spacy_fr = spacy.load('fr_core_news_sm')\n","\n","print(\"Loaded spaCy models\")\n","\n","# Example tokenization\n","en_text = \"Hello, how are you?\"\n","fr_text = \"Bonjour, comment allez-vous ?\"\n","\n","print(f\"EN tokens: {[token.text for token in spacy_en.tokenizer(en_text)]}\")\n","print(f\"FR tokens: {[token.text for token in spacy_fr.tokenizer(fr_text)]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEkMegILYffZ","executionInfo":{"status":"ok","timestamp":1762882053084,"user_tz":-60,"elapsed":5490,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"d635debd-2faa-4522-d409-08a31b6674e5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded spaCy models\n","EN tokens: ['Hello', ',', 'how', 'are', 'you', '?']\n","FR tokens: ['Bonjour', ',', 'comment', 'allez', '-vous', '?']\n"]}]},{"cell_type":"markdown","source":["### 3.3. Building the Vocabulary\n","\n","Our models don't understand words; they understand numbers. We need to create a \"vocabulary\" that maps every unique word in our dataset to a unique integer (index).\n","\n","We'll add four special tokens:\n","* `<unk>`: **Unknown** word. For any word we see in testing that wasn't in our training data.\n","* `<pad>`: **Padding**. We'll use this to make all sentences in a batch the same length.\n","* `<sos>`: **Start of Sentence**. A token to tell the decoder \"it's time to start generating!\"\n","* `<eos>`: **End of Sentence**. A token the decoder will learn to predict when it's finished."],"metadata":{"id":"ttMYpSPfYfPP"}},{"cell_type":"code","source":["class Vocabulary:\n","    def __init__(self, tokenizer, min_freq=2):\n","        self.tokenizer = tokenizer\n","        self.min_freq = min_freq\n","\n","        # Start with special tokens\n","        self.itos = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n","        self.stoi = {v: k for k, v in self.itos.items()}\n","\n","    def build_vocabulary(self, sentence_list):\n","        word_counts = Counter()\n","        for sentence in sentence_list:\n","            tokens = [token.text.lower() for token in self.tokenizer(sentence)]\n","            word_counts.update(tokens)\n","\n","        # Filter words by minimum frequency\n","        words = [word for word, count in word_counts.items() if count >= self.min_freq]\n","\n","        # Add filtered words to vocabulary\n","        idx = len(self.itos) # Start indexing from the end of special tokens\n","        for word in words:\n","            self.stoi[word] = idx\n","            self.itos[idx] = word\n","            idx += 1\n","\n","    def numericalize(self, text):\n","        tokens = [token.text.lower() for token in self.tokenizer(text)]\n","        return [self.stoi.get(token, self.stoi['<unk>']) for token in tokens]\n","\n","# 1. Load the data\n","pairs = []\n","with open(DATA_PATH, 'r', encoding='utf-8') as f:\n","    for line in f:\n","        # File has tab-separated pairs: EN\\tFR\\t...\n","        parts = line.strip().split('\\t')\n","        if len(parts) >= 2:\n","            pairs.append((parts[0], parts[1]))\n","\n","# For a small notebook, let's limit the dataset size to train faster\n","# Let's take 40,000 examples\n","pairs = pairs[:40000]\n","print(f\"Loaded {len(pairs)} sentence pairs.\")\n","\n","# 2. Separate source (EN) and target (FR)\n","source_sentences = [pair[0] for pair in pairs]\n","target_sentences = [pair[1] for pair in pairs]\n","\n","# 3. Build vocabularies\n","en_vocab = Vocabulary(spacy_en.tokenizer)\n","fr_vocab = Vocabulary(spacy_fr.tokenizer)\n","\n","en_vocab.build_vocabulary(source_sentences)\n","fr_vocab.build_vocabulary(target_sentences)\n","\n","print(f\"English (Source) Vocab Size: {len(en_vocab.itos)}\")\n","print(f\"French (Target) Vocab Size: {len(fr_vocab.itos)}\")\n","\n","# Example numericalization\n","example_text = \"I love this notebook.\"\n","print(f\"Original: {example_text}\")\n","numericalized = en_vocab.numericalize(example_text)\n","print(f\"Numericalized: {numericalized}\")\n","print(f\"Reversed: {[en_vocab.itos[i] for i in numericalized]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYrKuna0Yy6z","executionInfo":{"status":"ok","timestamp":1762882284015,"user_tz":-60,"elapsed":1575,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"9953183f-2561-4ec9-e18a-fa3d12abd006"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 40000 sentence pairs.\n","English (Source) Vocab Size: 3637\n","French (Target) Vocab Size: 5613\n","Original: I love this notebook.\n","Numericalized: [22, 335, 166, 3242, 5]\n","Reversed: ['i', 'love', 'this', 'notebook', '.']\n"]}]},{"cell_type":"markdown","source":["### 3.4. Creating the Dataset and DataLoader\n","\n","Now we'll create a PyTorch `Dataset` and `DataLoader`. This will handle batching our data (feeding it to the model in small chunks) and, importantly, **padding**.\n","\n","**Padding** is essential because RNNs in a batch need all sequences to be the same length. We'll add `<pad>` tokens to the end of shorter sentences."],"metadata":{"id":"0YpKANtEY2QU"}},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# Get special token indices\n","PAD_IDX = en_vocab.stoi['<pad>']\n","SOS_IDX = en_vocab.stoi['<sos>']\n","EOS_IDX = en_vocab.stoi['<eos>']\n","\n","class TranslationDataset(data.Dataset):\n","    def __init__(self, source_sentences, target_sentences, en_vocab, fr_vocab):\n","        self.source_data = []\n","        self.target_data = []\n","\n","        for i in range(len(source_sentences)):\n","            # Numericalize, add SOS/EOS tokens\n","            src = [SOS_IDX] + en_vocab.numericalize(source_sentences[i]) + [EOS_IDX]\n","            trg = [SOS_IDX] + fr_vocab.numericalize(target_sentences[i]) + [EOS_IDX]\n","\n","            self.source_data.append(torch.tensor(src))\n","            self.target_data.append(torch.tensor(trg))\n","\n","    def __len__(self):\n","        return len(self.source_data)\n","\n","    def __getitem__(self, idx):\n","        return self.source_data[idx], self.target_data[idx]\n","\n","# This custom 'collate_fn' is the key to batching\n","def collate_batch(batch):\n","    src_batch, trg_batch = [], []\n","    for src_sample, trg_sample in batch:\n","        src_batch.append(src_sample)\n","        trg_batch.append(trg_sample)\n","\n","    # Use pad_sequence to pad all items in a batch to the same length\n","    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n","    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n","\n","    return src_padded, trg_padded\n","\n","# --- Setup Dataloaders ---\n","\n","# Split data into train/validation\n","# (For a real project, use a dedicated test set)\n","total_size = len(pairs)\n","train_size = int(total_size * 0.9)\n","val_size = total_size - train_size\n","\n","train_pairs, val_pairs = data.random_split(pairs, [train_size, val_size])\n","\n","train_src = [pair[0] for pair in train_pairs]\n","train_trg = [pair[1] for pair in train_pairs]\n","val_src = [pair[0] for pair in val_pairs]\n","val_trg = [pair[1] for pair in val_pairs]\n","\n","train_dataset = TranslationDataset(train_src, train_trg, en_vocab, fr_vocab)\n","val_dataset = TranslationDataset(val_src, val_trg, en_vocab, fr_vocab)\n","\n","BATCH_SIZE = 128\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                          shuffle=True, collate_fn=collate_batch)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                        shuffle=False, collate_fn=collate_batch)\n","\n","print(f\"Created DataLoaders. {len(train_loader)} training batches.\")\n","\n","# Check a batch\n","src_batch, trg_batch = next(iter(train_loader))\n","print(f\"Source batch shape: {src_batch.shape}\") # (BATCH_SIZE, max_src_len)\n","print(f\"Target batch shape: {trg_batch.shape}\") # (BATCH_SIZE, max_trg_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEo9tNH3Y40V","executionInfo":{"status":"ok","timestamp":1762882343929,"user_tz":-60,"elapsed":2703,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"eda71182-5981-4b5d-faa2-f3ff34bcf9f5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Created DataLoaders. 282 training batches.\n","Source batch shape: torch.Size([128, 8])\n","Target batch shape: torch.Size([128, 12])\n"]}]},{"cell_type":"markdown","source":["## 4. The Architecture\n","\n","Time to build the model! We'll use a **GRU (Gated Recurrent Unit)**, which is a type of RNN similar to an LSTM but a bit simpler and faster.\n","\n","### 4.1. The Encoder\n","\n","The Encoder's job is to \"read\" the input sequence and output a single \"context vector.\"\n","\n","**Process:**\n","1.  **Embedding:** We convert the input word indices into dense vectors (this is what we learned about in the \"embeddings\" notebook).\n","2.  **RNN (GRU):** The embedded vectors are fed into the GRU one by one.\n","3.  **Output:** The GRU produces two things: `outputs` (the hidden state from *every* time step) and `hidden` (the *final* hidden state).\n","4.  **Context Vector:** This **final hidden state** (`hidden`) is our context vector. It's our \"summary\" of the entire input sentence."],"metadata":{"id":"8o68Yh4HY-9j"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.input_dim = input_dim\n","\n","        # 1. Embedding layer\n","        # input_dim = vocab size\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        # 2. GRU layer\n","        # We set batch_first=True to match our DataLoader\n","        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        # src shape = [batch_size, src_len]\n","\n","        # 1. Get embeddings\n","        embedded = self.dropout(self.embedding(src))\n","        # embedded shape = [batch_size, src_len, emb_dim]\n","\n","        # 2. Pass through RNN\n","        # outputs = hidden state for *every* token\n","        # hidden = the *final* hidden state\n","        outputs, hidden = self.rnn(embedded)\n","\n","        # outputs shape = [batch_size, src_len, hid_dim]\n","        # hidden shape = [1, batch_size, hid_dim] (1 for num_layers)\n","\n","        # The 'hidden' state is our context vector.\n","        # This is what we will pass to the decoder.\n","        return hidden"],"metadata":{"id":"Qv1_VQalZCyl","executionInfo":{"status":"ok","timestamp":1762882553172,"user_tz":-60,"elapsed":55,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 4.2. The Decoder\n","\n","The Decoder's job is to take the context vector and generate the output sequence, word by word.\n","\n","**Process:**\n","1.  **Input:** It takes the context vector from the encoder (as its *initial* hidden state), the *previous* word it generated (or `<sos>` to start), and gets embeddings.\n","2.  **RNN (GRU):** It runs the GRU for *one step* using the embedding and the *previous* hidden state.\n","3.  **Output:** The GRU produces a new `output` and a new `hidden` state.\n","4.  **Prediction:** The `output` vector (which has size `hid_dim`) is passed through a **Linear layer** to transform it into a vector the size of our *target vocabulary*.\n","5.  **Probabilities:** This final vector represents the \"scores\" for every word in the target (French) vocab. A softmax can turn this into probabilities. The word with the highest score is our prediction.\n","6.  **Loop:** The new `hidden` state is saved, and the predicted word is fed back in as the *input* for the next time step."],"metadata":{"id":"d5XjENuWZFUw"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.hid_dim = hid_dim\n","\n","        # 1. Embedding layer\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        # 2. GRU layer\n","        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n","\n","        # 3. Linear layer (to predict the next word)\n","        # It maps from hidden_dim to our target vocab size\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden):\n","        # This decoder processes one token at a time.\n","\n","        # input shape = [batch_size] (the current token)\n","        # hidden shape = [1, batch_size, hid_dim] (from encoder or last step)\n","\n","        # We need to add a sequence length dimension to the input\n","        input = input.unsqueeze(1) # shape = [batch_size, 1]\n","\n","        # 1. Get embeddings\n","        embedded = self.dropout(self.embedding(input))\n","        # embedded shape = [batch_size, 1, emb_dim]\n","\n","        # 2. Pass through RNN\n","        output, hidden = self.rnn(embedded, hidden)\n","        # output shape = [batch_size, 1, hid_dim]\n","        # hidden shape = [1, batch_size, hid_dim]\n","\n","        # 3. Pass through Linear layer\n","        # We remove the '1' dim from the output\n","        prediction = self.fc_out(output.squeeze(1))\n","        # prediction shape = [batch_size, output_dim]\n","\n","        return prediction, hidden"],"metadata":{"id":"NBeVXYf9ZHqz","executionInfo":{"status":"ok","timestamp":1762882769497,"user_tz":-60,"elapsed":23,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 4.3. The Seq2Seq Wrapper Model\n","\n","Finally, we create a \"wrapper\" model that combines the Encoder and Decoder. This model will manage the overall process.\n","\n","A key concept here is **Teacher Forcing**.\n","* **With Teacher Forcing (Training):** When training, instead of feeding the decoder's *own* prediction back in, we feed the **correct** word from the target data (e.g., the actual French sentence). This makes training much more stable and faster.\n","* **Without Teacher Forcing (Inference):** When *using* the model, we don't have the correct target data. So, we must feed the decoder's *own* prediction back in.\n","\n","Our model will use a `teacher_forcing_ratio` (e.g., 0.5) to randomly choose between these two methods during training."],"metadata":{"id":"u4BiRp04ZKlO"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # src = [batch_size, src_len]\n","        # trg = [batch_size, trg_len]\n","\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder's outputs\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","\n","        # 1. Run the encoder\n","        # context shape = [1, batch_size, hid_dim]\n","        context = self.encoder(src)\n","\n","        # 2. Run the decoder\n","        # The first input to the decoder is the <sos> token\n","        input = trg[:, 0] # Get all <sos> tokens from the batch\n","\n","        # The 'context' from the encoder is the first 'hidden' state\n","        # for the decoder\n","        hidden = context\n","\n","        # Loop from 1 (skip <sos>) to the end of the target sentence\n","        for t in range(1, trg_len):\n","\n","            # Run the decoder for one step\n","            output, hidden = self.decoder(input, hidden)\n","\n","            # Store the prediction\n","            outputs[:, t] = output\n","\n","            # Decide whether to \"teacher force\"\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            # Get the top predicted word\n","            top1 = output.argmax(1)\n","\n","            # If teacher forcing, use actual target word\n","            # Otherwise, use the decoder's own prediction\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs"],"metadata":{"id":"Kqucn4K0ZTv5","executionInfo":{"status":"ok","timestamp":1762882834329,"user_tz":-60,"elapsed":10,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## 5. Training the Model\n","\n","We're ready to train! We need to define our model, optimizer, and loss function.\n","\n","**The Loss Function:** We'll use **Cross-Entropy Loss**.\n","* It's perfect for this \"classification\" task (at each step, we're \"classifying\" the next word from all possibilities in the vocabulary).\n","* **Crucially:** We must tell the loss function to **ignore** the `<pad>` tokens. We don't want to penalize the model for its predictions on padding."],"metadata":{"id":"KRSxiC_lZWaP"}},{"cell_type":"code","source":["# --- Define Model Parameters ---\n","INPUT_DIM = len(en_vocab.itos)\n","OUTPUT_DIM = len(fr_vocab.itos)\n","EMB_DIM = 256\n","HID_DIM = 512\n","DROPOUT = 0.5\n","\n","# --- Instantiate Models ---\n","enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n","dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","# --- Optimizer ---\n","optimizer = optim.Adam(model.parameters())\n","\n","# --- Loss Function ---\n","# Get the index of our <pad> token\n","PAD_IDX = fr_vocab.stoi['<pad>']\n","\n","# Tell the loss function to ignore padding\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"],"metadata":{"id":"r64q36rnZYyQ","executionInfo":{"status":"ok","timestamp":1762882853556,"user_tz":-60,"elapsed":4978,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 5.1. The Training and Evaluation Loops\n","\n","We'll create a function for our training loop and one for our evaluation loop.\n","\n","**Important Note:** The `CrossEntropyLoss` function in PyTorch expects its input to be in a 2D shape: `(num_examples, num_classes)`. Our target is `(batch_size, seq_len)`. So, we have to `reshape` our outputs and targets before calculating the loss."],"metadata":{"id":"t5MD1H1hZTQ6"}},{"cell_type":"code","source":["def train_fn(model, loader, optimizer, criterion, clip):\n","    model.train() # Set model to training mode\n","    epoch_loss = 0\n","\n","    for i, (src, trg) in enumerate(loader):\n","        src = src.to(device)\n","        trg = trg.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # 1. Forward pass\n","        output = model(src, trg) # teacher_forcing_ratio is 0.5 by default\n","\n","        # output shape = [batch_size, trg_len, output_dim]\n","        # trg shape = [batch_size, trg_len]\n","\n","        # 2. Reshape for loss function\n","        # We skip the <sos> token (index 0)\n","        output_dim = output.shape[-1]\n","\n","        output_flat = output[:, 1:].reshape(-1, output_dim)\n","        trg_flat = trg[:, 1:].reshape(-1)\n","\n","        # output_flat shape = [(trg_len - 1) * batch_size, output_dim]\n","        # trg_flat shape = [(trg_len - 1) * batch_size]\n","\n","        # 3. Calculate loss\n","        loss = criterion(output_flat, trg_flat)\n","\n","        # 4. Backward pass and optimization\n","        loss.backward()\n","\n","        # Clip gradients to prevent them from exploding\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader)\n","\n","def evaluate_fn(model, loader, criterion):\n","    model.eval() # Set model to evaluation mode\n","    epoch_loss = 0\n","\n","    with torch.no_grad(): # No gradients needed for evaluation\n","        for i, (src, trg) in enumerate(loader):\n","            src = src.to(device)\n","            trg = trg.to(device)\n","\n","            # Forward pass (turn off teacher forcing)\n","            output = model(src, trg, teacher_forcing_ratio=0)\n","\n","            # Reshape for loss\n","            output_dim = output.shape[-1]\n","            output_flat = output[:, 1:].reshape(-1, output_dim)\n","            trg_flat = trg[:, 1:].reshape(-1)\n","\n","            # Calculate loss\n","            loss = criterion(output_flat, trg_flat)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader)"],"metadata":{"id":"AEoo5I2mZdUx","executionInfo":{"status":"ok","timestamp":1762882886563,"user_tz":-60,"elapsed":91,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### 5.2. Let's Train!\n","\n","Now we run the main loop. We'll also time it and save the model with the best validation loss."],"metadata":{"id":"tYMsvuCNZgg_"}},{"cell_type":"code","source":["N_EPOCHS = 30\n","CLIP = 1.0 # Gradient clipping value\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train_fn(model, train_loader, optimizer, criterion, CLIP)\n","    valid_loss = evaluate_fn(model, val_loader, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins = int((end_time - start_time) / 60)\n","    epoch_secs = int((end_time - start_time) % 60)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-model.pt') # Save the best model\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GS0Sc_DbZgBA","executionInfo":{"status":"ok","timestamp":1762883315721,"user_tz":-60,"elapsed":399921,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"2110d7d6-dfbd-4ced-b7aa-1f4297651b1f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 0m 12s\n","\tTrain Loss: 3.893 | Train PPL:  49.081\n","\t Val. Loss: 3.494 |  Val. PPL:  32.910\n","Epoch: 02 | Time: 0m 13s\n","\tTrain Loss: 2.833 | Train PPL:  17.000\n","\t Val. Loss: 3.062 |  Val. PPL:  21.369\n","Epoch: 03 | Time: 0m 13s\n","\tTrain Loss: 2.434 | Train PPL:  11.400\n","\t Val. Loss: 2.831 |  Val. PPL:  16.959\n","Epoch: 04 | Time: 0m 13s\n","\tTrain Loss: 2.163 | Train PPL:   8.695\n","\t Val. Loss: 2.684 |  Val. PPL:  14.645\n","Epoch: 05 | Time: 0m 13s\n","\tTrain Loss: 1.971 | Train PPL:   7.179\n","\t Val. Loss: 2.527 |  Val. PPL:  12.520\n","Epoch: 06 | Time: 0m 13s\n","\tTrain Loss: 1.786 | Train PPL:   5.963\n","\t Val. Loss: 2.526 |  Val. PPL:  12.498\n","Epoch: 07 | Time: 0m 13s\n","\tTrain Loss: 1.670 | Train PPL:   5.312\n","\t Val. Loss: 2.431 |  Val. PPL:  11.368\n","Epoch: 08 | Time: 0m 13s\n","\tTrain Loss: 1.553 | Train PPL:   4.725\n","\t Val. Loss: 2.327 |  Val. PPL:  10.247\n","Epoch: 09 | Time: 0m 13s\n","\tTrain Loss: 1.454 | Train PPL:   4.278\n","\t Val. Loss: 2.327 |  Val. PPL:  10.248\n","Epoch: 10 | Time: 0m 13s\n","\tTrain Loss: 1.375 | Train PPL:   3.955\n","\t Val. Loss: 2.277 |  Val. PPL:   9.750\n","Epoch: 11 | Time: 0m 13s\n","\tTrain Loss: 1.308 | Train PPL:   3.698\n","\t Val. Loss: 2.225 |  Val. PPL:   9.253\n","Epoch: 12 | Time: 0m 13s\n","\tTrain Loss: 1.226 | Train PPL:   3.409\n","\t Val. Loss: 2.256 |  Val. PPL:   9.541\n","Epoch: 13 | Time: 0m 13s\n","\tTrain Loss: 1.185 | Train PPL:   3.270\n","\t Val. Loss: 2.217 |  Val. PPL:   9.182\n","Epoch: 14 | Time: 0m 13s\n","\tTrain Loss: 1.157 | Train PPL:   3.180\n","\t Val. Loss: 2.199 |  Val. PPL:   9.015\n","Epoch: 15 | Time: 0m 13s\n","\tTrain Loss: 1.106 | Train PPL:   3.022\n","\t Val. Loss: 2.221 |  Val. PPL:   9.221\n","Epoch: 16 | Time: 0m 13s\n","\tTrain Loss: 1.058 | Train PPL:   2.879\n","\t Val. Loss: 2.217 |  Val. PPL:   9.181\n","Epoch: 17 | Time: 0m 13s\n","\tTrain Loss: 1.032 | Train PPL:   2.808\n","\t Val. Loss: 2.226 |  Val. PPL:   9.266\n","Epoch: 18 | Time: 0m 13s\n","\tTrain Loss: 0.997 | Train PPL:   2.711\n","\t Val. Loss: 2.230 |  Val. PPL:   9.299\n","Epoch: 19 | Time: 0m 13s\n","\tTrain Loss: 0.962 | Train PPL:   2.616\n","\t Val. Loss: 2.249 |  Val. PPL:   9.478\n","Epoch: 20 | Time: 0m 13s\n","\tTrain Loss: 0.958 | Train PPL:   2.608\n","\t Val. Loss: 2.214 |  Val. PPL:   9.154\n","Epoch: 21 | Time: 0m 13s\n","\tTrain Loss: 0.949 | Train PPL:   2.583\n","\t Val. Loss: 2.251 |  Val. PPL:   9.501\n","Epoch: 22 | Time: 0m 13s\n","\tTrain Loss: 0.924 | Train PPL:   2.520\n","\t Val. Loss: 2.232 |  Val. PPL:   9.323\n","Epoch: 23 | Time: 0m 13s\n","\tTrain Loss: 0.887 | Train PPL:   2.427\n","\t Val. Loss: 2.274 |  Val. PPL:   9.723\n","Epoch: 24 | Time: 0m 13s\n","\tTrain Loss: 0.870 | Train PPL:   2.387\n","\t Val. Loss: 2.278 |  Val. PPL:   9.754\n","Epoch: 25 | Time: 0m 13s\n","\tTrain Loss: 0.865 | Train PPL:   2.376\n","\t Val. Loss: 2.284 |  Val. PPL:   9.820\n","Epoch: 26 | Time: 0m 12s\n","\tTrain Loss: 0.865 | Train PPL:   2.375\n","\t Val. Loss: 2.268 |  Val. PPL:   9.657\n","Epoch: 27 | Time: 0m 13s\n","\tTrain Loss: 0.838 | Train PPL:   2.312\n","\t Val. Loss: 2.298 |  Val. PPL:   9.957\n","Epoch: 28 | Time: 0m 13s\n","\tTrain Loss: 0.827 | Train PPL:   2.287\n","\t Val. Loss: 2.295 |  Val. PPL:   9.925\n","Epoch: 29 | Time: 0m 13s\n","\tTrain Loss: 0.804 | Train PPL:   2.234\n","\t Val. Loss: 2.327 |  Val. PPL:  10.249\n","Epoch: 30 | Time: 0m 13s\n","\tTrain Loss: 0.824 | Train PPL:   2.280\n","\t Val. Loss: 2.288 |  Val. PPL:   9.851\n"]}]},{"cell_type":"markdown","source":["## 6. Inference: Using Our Model\n","\n","Training is done! Now for the fun part: using our model to translate.\n","\n","For inference, we **cannot** use teacher forcing. We must feed the model's *own* predictions back into it. This process is called \"greedy decoding\" (we always pick the word with the single highest probability).\n","\n","We'll write a function that:\n","1.  Takes a new English sentence.\n","2.  Tokenizes and numericalizes it.\n","3.  Feeds it to the **Encoder** to get a `context` vector.\n","4.  Starts the **Decoder** with the `<sos>` token.\n","5.  Loops, feeding the *last predicted word* back into the decoder.\n","6.  Stops when the decoder predicts `<eos>` or we hit a max length.\n","7.  Converts the output indices back into French words."],"metadata":{"id":"z5R4YLicZfoC"}},{"cell_type":"code","source":["# Load our best saved model\n","model.load_state_dict(torch.load('best-model.pt'))\n","\n","def translate_sentence(sentence, model, en_vocab, fr_vocab, device, max_len=50):\n","    model.eval() # Set to evaluation mode\n","\n","    # 1. Tokenize and numericalize\n","    tokens = [token.text.lower() for token in spacy_en.tokenizer(sentence)]\n","    tokens = [SOS_IDX] + [en_vocab.stoi.get(t, en_vocab.stoi['<unk>']) for t in tokens] + [EOS_IDX]\n","\n","    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device) # [1, src_len]\n","\n","    with torch.no_grad():\n","        # 2. Get context vector from encoder\n","        context = model.encoder(src_tensor) # [1, 1, hid_dim]\n","\n","    # 3. Start decoder with <sos>\n","    trg_indices = [SOS_IDX]\n","\n","    # The context is the first hidden state\n","    hidden = context\n","\n","    for _ in range(max_len):\n","        # Get the last predicted word\n","        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)\n","\n","        # 4. Run decoder for one step\n","        output, hidden = model.decoder(trg_tensor, hidden)\n","\n","        # 5. Get the top prediction\n","        pred_token = output.argmax(1).item()\n","\n","        trg_indices.append(pred_token)\n","\n","        # 6. Stop if <eos>\n","        if pred_token == EOS_IDX:\n","            break\n","\n","    # 7. Convert indices back to words\n","    trg_tokens = [fr_vocab.itos[i] for i in trg_indices]\n","\n","    # Return the translation (skipping <sos>)\n","    return \" \".join(trg_tokens[1:])\n","\n","# --- Let's try some examples! ---\n","\n","# Pick a random sentence from the validation set\n","example_idx = 10\n","src_sentence = val_src[example_idx]\n","trg_sentence = val_trg[example_idx]\n","\n","translation = translate_sentence(src_sentence, model, en_vocab, fr_vocab, device)\n","\n","print(f\"Source (EN): {src_sentence}\")\n","print(f\"Target (FR): {trg_sentence}\")\n","print(f\"Model (PRED): {translation}\")\n","print(\"---\")\n","\n","# Try a custom sentence\n","custom_sentence = \"A man is reading a book.\"\n","translation = translate_sentence(custom_sentence, model, en_vocab, fr_vocab, device)\n","\n","print(f\"Source (EN): {custom_sentence}\")\n","print(f\"Model (PRED): {translation}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfWao4WkZn3C","executionInfo":{"status":"ok","timestamp":1762883315907,"user_tz":-60,"elapsed":146,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"51a2b44c-68eb-4fb1-a833-7ce71a7114d1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Source (EN): I'm right here.\n","Target (FR): Je suis juste ici.\n","Model (PRED): je suis ici ici . <eos>\n","---\n","Source (EN): A man is reading a book.\n","Model (PRED): un mouche est un . <eos>\n"]}]},{"cell_type":"markdown","source":["## 7. Conclusion and Next Steps\n","\n","Congratulations! You've just built a complete sequence-to-sequence model from scratch.\n","\n","You'll notice the translations are... *okay*, but not perfect. They might be repetitive or grammatically awkward. This is typical for a \"vanilla\" Seq2Seq model.\n","\n","**The Bottleneck Problem:** Our model's main weakness is the **context vector**. The encoder has to compress the *entire* meaning of a 20-word sentence into one small vector (`hidden`). This is a huge information bottleneck!\n","\n","**The Solution:** The next step is to implement an **Attention** mechanism.\n","\n","**Attention** allows the decoder to \"look back\" at *all* of the encoder's outputs (not just the final hidden state) at every step of the generation process. It learns to \"pay attention\" to the most relevant input words when generating the next output word.\n","\n","\n","\n","This single improvement is what made Seq2Seq models state-of-the-art and is a direct precursor to the Transformer (which is 100% attention).\n","\n","This notebook provides the perfect foundation for you to build an \"Attention\" model next!"],"metadata":{"id":"Tc9r640BZqXt"}}]}
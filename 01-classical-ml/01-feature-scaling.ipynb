{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19fbafe",
   "metadata": {},
   "source": [
    "# Feature Scaling: Normalization and Standardization\n",
    "\n",
    "**Feature scaling** is a data preprocessing technique used to **resize the numerical values of features** to a similar range or to give them common statistical properties. This is done before training a machine learning model to ensure that all features contribute equally to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd25ac",
   "metadata": {},
   "source": [
    "### âœ… Why is Feature Scaling Important?\n",
    "\n",
    "1.  **Distance-Based Algorithms**: Many algorithms like k-NN, k-Means, and SVMs rely on distance calculations (e.g., Euclidean distance). If one feature has a much larger range than others, it will dominate the distance calculation, making the model biased towards that feature.\n",
    "2.  **Gradient Descent Convergence**: Algorithms that use gradient descent (like neural networks and logistic regression) converge much faster when features are on a similar scale. This is because the optimization landscape becomes more uniform, preventing skewed and slow gradient updates.\n",
    "3.  **Regularization**: Techniques like L1 and L2 regularization apply penalties to model weights. If features have different scales, their corresponding weights will be updated unevenly, making the regularization less effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc5ff3",
   "metadata": {},
   "source": [
    "### Original Data\n",
    "\n",
    "Let's start with a simple dataset containing 'Age' and 'Salary'. As you can see, the scale of 'Salary' is much larger than 'Age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3373056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Age  Salary\n",
      "0   30   20000\n",
      "1   40   80000\n",
      "2   50   50000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Age': [30, 40, 50],\n",
    "    'Salary': [20000, 80000, 50000]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16385e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a2de0",
   "metadata": {},
   "source": [
    "## 1. Min-Max Scaling (Normalization)\n",
    "\n",
    "This technique rescales the data to a fixed range, usually **[0, 1]**. It's called \"normalization\" because it normalizes the data within a bounded interval.\n",
    "\n",
    "The formula is:\n",
    "$$ X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fd976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after Min-Max Scaling:\n",
      "   Age  Salary\n",
      "0  0.0     0.0\n",
      "1  0.5     1.0\n",
      "2  1.0     0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Min-Max Scaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "min_max_scaled_data = mm_scaler.fit_transform(data)\n",
    "\n",
    "print(\"Data after Min-Max Scaling:\")\n",
    "print(pd.DataFrame(min_max_scaled_data, columns=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08a00a",
   "metadata": {},
   "source": [
    "**When to use it?** Good for algorithms that don't assume a specific data distribution (like k-NN or neural networks). It's especially useful when you need your feature values to be bounded within a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0882172",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09dcc0",
   "metadata": {},
   "source": [
    "## 2. Standardization (Z-score Scaling)\n",
    "\n",
    "This technique rescales the data so that it has a **mean ($\\mu$) of 0 and a standard deviation ($\\sigma$) of 1**. The resulting distribution is known as a standard normal distribution.\n",
    "\n",
    "The formula is:\n",
    "$$ X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f755330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after Standardization:\n",
      "        Age    Salary\n",
      "0 -1.224745 -1.224745\n",
      "1  0.000000  1.224745\n",
      "2  1.224745  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Standard Scaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "standardized_data = std_scaler.fit_transform(data)\n",
    "\n",
    "print(\"Data after Standardization:\")\n",
    "print(pd.DataFrame(standardized_data, columns=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef7725",
   "metadata": {},
   "source": [
    "**When to use it?** Works very well for algorithms that assume a Gaussian (normal) distribution of the features, such as Linear Regression, Logistic Regression, and SVMs. It is also less sensitive to outliers compared to Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51907949",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f2a97",
   "metadata": {},
   "source": [
    "## Summary: Which One to Choose?\n",
    "\n",
    "| Method | Formula | Resulting Range | Key Use Case | Sensitivity to Outliers |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Min-Max Scaling** | `(X - min) / (max - min)` | Typically [0, 1] | Good when you need bounded values, e.g., for image processing or some neural networks. | High |\n",
    "| **Standardization** | `(X - mean) / std_dev` | Not bounded | Crucial for distance-based algorithms like SVMs (with RBF kernel) | Low |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

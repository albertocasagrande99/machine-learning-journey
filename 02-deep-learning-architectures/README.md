# ðŸ“š 02-deep-learning-architectures: Modern Neural Network Architectures
This folder contains hands-on notebooks that dive into important deep learning architectures. Each notebook mixes theory, worked examples, and runnable PyTorch code. The notebooks use MNIST for many image examples and smaller synthetic or demo datasets for faster experimentation.

## ðŸŽ¯ Notebooks
| \# | Notebook | Topic | Key concepts & highlights |
| --- | --- | --- | --- |
| 1 | `01-multi-layer-perceptrons.ipynb` | Multi-Layer Perceptrons (MLPs) | Perceptron to MLPs, activation functions (sigmoid, tanh, ReLU, LeakyReLU, softmax), forward/backward math, implement an MLP from scratch (NumPy) and practical usage with scikit-learn's `MLPClassifier` and a small PyTorch example. Includes XOR demo, visualization and hyperparameter tuning. |
| 2 | `02-optimization-techniques.ipynb` | Optimization Techniques | Gradient Descent and variants (Batch / Mini-batch / SGD), Momentum & Nesterov, adaptive optimizers (Adagrad, RMSprop, Adam), learning-rate schedules, visualization of optimizer paths, and a practical PyTorch comparison training an MLP. |
| 3 | `03-regularization-techniques.ipynb` | Regularization Techniques | The overfitting problem and remedies: L1/L2 (weight decay), Dropout, Batch Normalization, Early Stopping, and brief data-augmentation notes. Includes PyTorch experiments comparing regularization effects. |
| 4 | `04-convolutional-neural-networks.ipynb` | Convolutional Neural Networks (CNNs) â€” MNIST | Intuition and formulas for 2D convolutions, padding/stride, channels and receptive field. Full PyTorch MNIST tutorial: DataLoader, simple CNN (Convâ€“BNâ€“ReLU blocks), training loop, schedulers & checkpointing, and visualization of filters & feature maps. |
| 5 | `05-recurrent-neural-networks.ipynb` | Recurrent Neural Networks (RNNs) / LSTMs â€” PyTorch NLP | Lightweight NLP pipeline with tokenization, `Vocab`, padding/collate utilities, a `TextDataset`, and a compact Embedding + LSTM classifier (supports pack_padded_sequence and bidirectional LSTMs). Includes training/evaluation utilities, a mini synthetic demo, checkpoint saving/loading, and inference examples. |
| 6 | `06-graph-neural-networks.ipynb` | Graph Neural Networks (GNNs) â€” Cora | Message-passing intuition and the GCN formula, installing/using PyTorch Geometric (PyG), loading the Cora Planetoid dataset, building a 2-layer `GCNConv` model, full-batch training with train/val/test masks, and inspecting node-level predictions. |

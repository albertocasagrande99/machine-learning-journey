{"cells":[{"cell_type":"markdown","id":"eba58343","metadata":{"id":"eba58343"},"source":["# PyTorch NLP: training an RNN (LSTM) for text classification\n","\n","**Contents**\n","- Prerequisites & installation\n","- Imports and lightweight preprocessing utilities (tokenization, Vocab, Dataset)\n","- Mini synthetic demo dataset (quick smoke test)\n","- Model: Embedding + LSTM classifier (handling padded sequences)\n","- Training and evaluation utilities (train/evaluate loops)\n","- Training run, saving checkpoint, and inference on new sentences\n"]},{"cell_type":"markdown","id":"909268c6","metadata":{"id":"909268c6"},"source":["## Prerequisites\n","\n","If you work in a fresh environment, install PyTorch and (optionally) `torchtext`. Example (CPU-only):\n","```bash\n","# Install PyTorch + torchtext (example). Choose the correct command for your CUDA version on https://pytorch.org\n","pip install torch torchvision torchaudio torchtext nbformat\n","```\n","\n","This notebook uses:\n","- `torch`, `torch.nn`, `torch.utils.data` for model and training utilities\n","- `torchtext` only optionally for larger datasets (not required for the mini-demo)\n","- `numpy`, `matplotlib` for convenience/visualization\n"]},{"cell_type":"code","execution_count":3,"id":"5e30bf5f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5e30bf5f","executionInfo":{"status":"ok","timestamp":1760037942090,"user_tz":-120,"elapsed":3986,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"eeefbf76-8259-4e1a-b64d-c771015634ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch version: 2.8.0+cu126\n","torchtext available: False\n"]}],"source":["# Basic imports and optional torchtext check\n","import os\n","import random\n","from collections import Counter\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Optional: check torchtext availability (not required for the mini demo)\n","try:\n","    import torchtext\n","    from torchtext.datasets import IMDB\n","    TORCHTEXT_AVAILABLE = True\n","except Exception:\n","    TORCHTEXT_AVAILABLE = False\n","\n","print('torch version:', torch.__version__)\n","print('torchtext available:', TORCHTEXT_AVAILABLE)"]},{"cell_type":"markdown","id":"c343aa76","metadata":{"id":"c343aa76"},"source":["## Utilities — tokenization, vocabulary, dataset and batching\n","\n","This section defines lightweight utilities to convert raw text into tensors suitable for PyTorch models:\n","\n","- `simple_tokenize(text)` — minimal tokenizer that lowercases and splits on whitespace.  `\"I Loved the Movie\"` → `[\"i\", \"loved\", \"the\", \"movie\"]`\n","- `Vocab` — builds a mapping token ↔ index, tracks frequencies, supports `min_freq`, and provides `numericalize()`.\n","- `TextDataset` — a `torch.utils.data.Dataset` that returns `(tensor_of_indices, label)` for each example.\n","- `collate_batch(batch)` — collate function for `DataLoader` that pads sequences to the same length and returns `(padded_sequences, lengths, labels)`.\n","\n","These utilities are intentionally simple and easy to understand. For production or larger datasets, consider using `torchtext`, `huggingface/tokenizers`, or other robust tokenization libraries.\n"]},{"cell_type":"code","execution_count":4,"id":"ea1b45d8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ea1b45d8","executionInfo":{"status":"ok","timestamp":1760037942224,"user_tz":-120,"elapsed":67,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"56af3d37-3c31-4ea9-f1fb-bee7ffaa0432"},"outputs":[{"output_type":"stream","name":"stdout","text":["Utilities defined\n"]}],"source":["# Minimal tokenizer\n","def simple_tokenize(text: str):\n","    \"\"\"Very small tokenizer: lowercases and splits on spaces.\"\"\"\n","    return text.lower().strip().split()\n","\n","# Vocabulary class\n","from collections import Counter\n","\n","class Vocab:\n","    def __init__(self, min_freq: int = 1, specials=['<pad>', '<unk>']):\n","        self.freqs = Counter()\n","        self.itos = list(specials)                 # index -> token\n","        self.stoi = {tok: i for i, tok in enumerate(self.itos)}  # token -> index\n","        self.min_freq = min_freq\n","\n","    def build(self, iterator):\n","        \"\"\"Build vocabulary from an iterator of token lists.\"\"\"\n","        for tokens in iterator:\n","            self.freqs.update(tokens)\n","        # Add tokens sorted by frequency (descending)\n","        for tok, cnt in sorted(self.freqs.items(), key=lambda x: -x[1]):\n","            if cnt >= self.min_freq and tok not in self.stoi:\n","                self.stoi[tok] = len(self.itos)\n","                self.itos.append(tok)\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    def numericalize(self, tokens):\n","        \"\"\"Convert a list of tokens to a list of indices (unk -> <unk>).\"\"\"\n","        # If a token is not in vocabulary, return the index of <unk>\n","        unk_idx = self.stoi.get('<unk>')\n","        return [self.stoi.get(t, unk_idx) for t in tokens]\n","\n","# Dataset class for text classification\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels, vocab: Vocab = None):\n","        assert len(texts) == len(labels)\n","        self.texts = texts\n","        self.labels = labels\n","        if vocab is None:\n","            toks_iter = (simple_tokenize(t) for t in texts)\n","            self.vocab = Vocab()\n","            self.vocab.build(toks_iter)\n","        else:\n","            self.vocab = vocab\n","\n","    def __len__(self):\n","        # number of examples in the dataset\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = simple_tokenize(self.texts[idx]) # tokenize\n","        nums = self.vocab.numericalize(tokens)    # numericalize (convert tokens to indices)\n","        label = self.labels[idx]                  # get corresponding label\n","        return torch.tensor(nums, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","\n","# collate_fn for DataLoader: pads sequences and returns lengths\n","from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_batch(batch):\n","    sequences, labels = zip(*batch)\n","    lengths = torch.tensor([len(s) for s in sequences], dtype=torch.long) # length of each sequence\n","    padded = pad_sequence(sequences, batch_first=True, padding_value=0) # sequence padding\n","    labels = torch.stack(labels)\n","    return padded, lengths, labels\n","\n","print('Utilities defined')"]},{"cell_type":"markdown","id":"4b10ba53","metadata":{"id":"4b10ba53"},"source":["## Mini synthetic demo (quick smoke test)\n","\n","We create a tiny synthetic dataset (7 short sentences) labeled with binary sentiment:\n","- `1` → positive\n","- `0` → negative\n","\n","The dataset is shuffled, split into train/validation and wrapped by `TextDataset` and `DataLoader` using `collate_batch`. This allows us to do a quick end-to-end run to verify the model and training pipeline work.\n"]},{"cell_type":"code","execution_count":6,"id":"b34d67d5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b34d67d5","executionInfo":{"status":"ok","timestamp":1760038396425,"user_tz":-120,"elapsed":28,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"a253c796-be8c-429c-8267-238b1ffbf32e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size (mini-demo): 30\n"]}],"source":["# Tiny dataset: 7 sentences (labels: 1=positive, 0=negative)\n","texts = [\n","    'I loved the movie it was fantastic and fun',      # positive\n","    'What a terrible movie it was boring and slow',    # negative\n","    'Amazing plot and great characters',               # positive\n","    'I hated it worst film ever',                      # negative\n","    'It was okay not great but not bad',               # positive/neutral\n","    'An outstanding masterpiece of cinema',            # positive\n","    'Awful acting and poor script',                    # negative\n","]\n","labels = [1, 0, 1, 0, 1, 1, 0]\n","\n","# Shuffle and split into train (5) / val (2)\n","data = list(zip(texts, labels))\n","random.shuffle(data)\n","train = data[:5]\n","val = data[5:]\n","\n","train_texts, train_labels = zip(*train)\n","val_texts, val_labels = zip(*val)\n","\n","# Create datasets and loaders\n","train_ds = TextDataset(list(train_texts), list(train_labels))\n","val_ds = TextDataset(list(val_texts), list(val_labels), vocab=train_ds.vocab)  # use same vocab as train\n","\n","train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_batch) # each batch contains two sentences\n","val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=collate_batch)\n","\n","print('Vocab size (mini-demo):', len(train_ds.vocab))"]},{"cell_type":"markdown","id":"903bc495","metadata":{"id":"903bc495"},"source":["## Model: Embedding + LSTM classifier\n","\n","Architecture details:\n","1. **Embedding layer**: maps token indices to dense vectors (`embed_dim`). We set `padding_idx=0` so `<pad>` embeddings remain zeros.\n","2. **LSTM**: can be multi-layer and bidirectional. We use `pack_padded_sequence` to efficiently handle padded sequences.\n","3. **Final hidden state**: for a bidirectional LSTM we concatenate final forward and backward hidden states. For a unidirectional LSTM we use the last hidden state.\n","4. **Dropout + Linear**: apply dropout and a final `Linear` to produce logits for classes. We return raw logits (use `CrossEntropyLoss`).\n","\n","The forward method accepts `(padded_sequences, lengths)` and returns `(batch, num_classes)` logits.\n"]},{"cell_type":"code","execution_count":7,"id":"b1cd6478","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1cd6478","executionInfo":{"status":"ok","timestamp":1760038746587,"user_tz":-120,"elapsed":22,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"8c438fe2-5863-4e08-f468-2cb709be03d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model class defined\n"]}],"source":["class RNNClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128, num_layers=1, bidirectional=True,\n","                 num_classes=2, dropout=0.3):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(input_size=embed_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=num_layers,\n","                            batch_first=True,\n","                            bidirectional=bidirectional,\n","                            dropout=dropout if num_layers > 1 else 0.0)\n","        self.bidirectional = bidirectional\n","        self.num_directions = 2 if bidirectional else 1\n","        self.fc = nn.Linear(hidden_dim * self.num_directions, num_classes)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, lengths):\n","        # x: (batch, seq_len)\n","        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n","        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n","        packed_out, (h_n, c_n) = self.lstm(packed)\n","        # h_n shape: (num_layers * num_directions, batch, hidden_dim)\n","        if self.bidirectional:\n","            # last layer forward = -2, last layer backward = -1\n","            h_forward = h_n[-2, :, :]\n","            h_backward = h_n[-1, :, :]\n","            h = torch.cat((h_forward, h_backward), dim=1)  # (batch, hidden_dim*2)\n","        else:\n","            h = h_n[-1, :, :]  # (batch, hidden_dim)\n","        out = self.dropout(h)\n","        logits = self.fc(out)\n","        return logits\n","\n","print('Model class defined')"]},{"cell_type":"markdown","id":"0e13d04b","metadata":{"id":"0e13d04b"},"source":["## Training and Evaluation utilities\n","\n","We define `train_epoch` and `evaluate` helper functions that:\n","- run one training epoch (forward, backward, optimizer step),\n","- compute and return average loss and accuracy,\n","- evaluate the model on a validation/test loader without updating weights.\n"]},{"cell_type":"code","execution_count":11,"id":"f2e51e95","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2e51e95","executionInfo":{"status":"ok","timestamp":1760039177273,"user_tz":-120,"elapsed":40,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"f5deb5a3-f062-46dc-c463-7442c15eb3d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training utilities defined\n"]}],"source":["def train_epoch(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for xb, lengths, yb in dataloader:\n","        xb, lengths, yb = xb.to(device), lengths.to(device), yb.to(device)\n","        optimizer.zero_grad() # reset the gradient\n","        logits = model(xb, lengths) # compute logits\n","        loss = criterion(logits, yb) # compute loss by comparing logits with targets\n","        loss.backward() # backpropagation\n","        optimizer.step() # update weights\n","        total_loss += loss.item() * xb.size(0) # accumulate loss over the batch\n","        preds = logits.argmax(dim=1) # predictions: class with the highest probability\n","        correct += (preds == yb).sum().item() # number of correct predictions\n","        total += xb.size(0) # total number of examples\n","    return total_loss / total, correct / total\n","\n","\n","def evaluate(model, dataloader, criterion, device):\n","    model.eval() # disable dropout\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad(): # avoids computing gradients\n","        for xb, lengths, yb in dataloader:\n","            xb, lengths, yb = xb.to(device), lengths.to(device), yb.to(device)\n","            logits = model(xb, lengths)\n","            loss = criterion(logits, yb)\n","            total_loss += loss.item() * xb.size(0)\n","            preds = logits.argmax(dim=1)\n","            correct += (preds == yb).sum().item()\n","            total += xb.size(0)\n","    return total_loss / total, correct / total\n","\n","print('Training utilities defined')"]},{"cell_type":"markdown","id":"b54b3299","metadata":{"id":"b54b3299"},"source":["## Training run: train for a few epochs and save a checkpoint\n","\n","This cell:\n","- prepares the device,\n","- instantiates the model with reduced sizes for a fast demo (`embed_dim=32, hidden_dim=32`),\n","- trains for a small number of epochs (5 epoch in the mini-demo),\n","- saves a checkpoint dict with `model_state_dict` and `vocab.itos` so the model can be reconstructed later.\n"]},{"cell_type":"code","execution_count":12,"id":"8ccbf290","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ccbf290","executionInfo":{"status":"ok","timestamp":1760039201327,"user_tz":-120,"elapsed":6310,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"cadd1526-e518-440b-b4ef-80a2319f1cdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","RNNClassifier(\n","  (embedding): Embedding(30, 32, padding_idx=0)\n","  (lstm): LSTM(32, 32, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=64, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")\n","Epoch 01 | train loss 0.6928 acc 0.6000 | val loss 0.6907 acc 0.5000\n","Epoch 02 | train loss 0.6391 acc 0.6000 | val loss 0.6961 acc 0.0000\n","Epoch 03 | train loss 0.6169 acc 0.6000 | val loss 0.7032 acc 0.0000\n","Epoch 04 | train loss 0.5858 acc 1.0000 | val loss 0.7100 acc 0.0000\n","Epoch 05 | train loss 0.5613 acc 1.0000 | val loss 0.7176 acc 0.0000\n","Model saved to rnn_classifier_mini.pth\n"]}],"source":["# Device setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Device:', device)\n","\n","# Instantiate model (small for demo)\n","vocab_size = len(train_ds.vocab)\n","model = RNNClassifier(vocab_size=vocab_size, embed_dim=32, hidden_dim=32, num_layers=1,\n","                      bidirectional=True, num_classes=2, dropout=0.2)\n","print(model)\n","model.to(device)\n","\n","# Optimizer & loss\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Quick training (5 epoch for demo)\n","num_epochs = 5\n","for epoch in range(1, num_epochs + 1):\n","    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n","    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","    print(f'Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc:.4f} | val loss {val_loss:.4f} acc {val_acc:.4f}')\n","\n","# Save checkpoint\n","ckpt_path = 'rnn_classifier_mini.pth'\n","torch.save({'model_state_dict': model.state_dict(), 'vocab': train_ds.vocab.itos}, ckpt_path)\n","print('Model saved to', ckpt_path)"]},{"cell_type":"markdown","id":"4c8c95ed","metadata":{"id":"4c8c95ed"},"source":["## Inference: load the checkpoint and classify a new sentence\n","\n","This cell demonstrates how to:\n","- load the saved checkpoint,\n","- reconstruct a `Vocab` from the saved `itos`,\n","- infer model hyperparameters (embedding dim, hidden dim, bidirectionality) from the saved state dict,\n","- rebuild the `RNNClassifier`, load weights, and run a prediction on a new sentence.\n"]},{"cell_type":"code","execution_count":15,"id":"e20455b1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e20455b1","executionInfo":{"status":"ok","timestamp":1760039275145,"user_tz":-120,"elapsed":52,"user":{"displayName":"Alberto Casagrande","userId":"03712360769530450037"}},"outputId":"fb07f97f-1b5e-4e8f-fa08-9be8327b6c3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['i', 'really', 'enjoyed', 'the', 'film', '—', 'it', 'was', 'delightful', 'and', 'fun']\n","Indices: [7, 1, 1, 9, 1, 1, 2, 3, 1, 4, 11]\n","Predicted label: 1 (class probs: [0.44648206 0.553518  ])\n","Human-readable: positive\n"]}],"source":["# Load checkpoint and reconstruct model for inference\n","from pathlib import Path\n","ckpt_path = Path('rnn_classifier_mini.pth')\n","assert ckpt_path.exists(), f'Checkpoint not found: {ckpt_path} (run training cell first)'\n","\n","ckpt = torch.load(ckpt_path, map_location='cpu')\n","state_dict = ckpt['model_state_dict']\n","itos = ckpt.get('vocab', None)\n","assert itos is not None, 'Checkpoint does not contain saved vocab (itos)'\n","\n","# Rebuild Vocab object\n","vocab_obj = Vocab()\n","vocab_obj.itos = list(itos)\n","vocab_obj.stoi = {tok: i for i, tok in enumerate(vocab_obj.itos)}\n","\n","# Infer embedding size and vocab size from saved weights\n","emb_weight = state_dict['embedding.weight']\n","vocab_size = emb_weight.shape[0]\n","embed_dim = emb_weight.shape[1]\n","\n","# Inspect LSTM keys to infer hidden size, num_layers and bidirectionality\n","lstm_keys = [k for k in state_dict.keys() if k.startswith('lstm.')]\n","bidirectional = any('weight_ih_l0_reverse' in k or 'weight_hh_l0_reverse' in k for k in lstm_keys)\n","w_ih_l0 = state_dict.get('lstm.weight_ih_l0')\n","assert w_ih_l0 is not None, 'Expected lstm.weight_ih_l0 key in state_dict'\n","hidden_dim = w_ih_l0.shape[0] // 4\n","\n","layer_indices = set()\n","import re\n","for k in lstm_keys:\n","    m = re.search(r'lstm\\.weight_ih_l(\\d+)', k)\n","    if m:\n","        layer_indices.add(int(m.group(1)))\n","num_layers = max(layer_indices) + 1 if layer_indices else 1\n","\n","# Recreate and load model\n","model = RNNClassifier(vocab_size=vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim,\n","                      num_layers=num_layers, bidirectional=bidirectional, num_classes=2, dropout=0.2)\n","model.load_state_dict(state_dict)\n","model.to(device)\n","model.eval()\n","\n","# Single sentence prediction example\n","sentence = \"I really enjoyed the film — it was delightful and fun\"\n","tokens = simple_tokenize(sentence)\n","indices = vocab_obj.numericalize(tokens)\n","print('Tokens:', tokens)\n","print('Indices:', indices)\n","\n","xb = torch.tensor([indices], dtype=torch.long)\n","lengths = torch.tensor([len(indices)], dtype=torch.long)\n","with torch.no_grad():\n","    logits = model(xb.to(device), lengths.to(device))\n","    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n","    pred = int(probs.argmax())\n","    print(f'Predicted label: {pred} (class probs: {probs})')\n","\n","label_map = {0: 'negative', 1: 'positive'}\n","print('Human-readable:', label_map.get(pred, str(pred)))"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}